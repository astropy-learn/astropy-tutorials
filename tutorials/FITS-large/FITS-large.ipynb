{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd63a37-6a0c-4c92-a970-3e2ad24cbde2",
   "metadata": {},
   "source": [
    "# Working with large FITS files\n",
    "\n",
    "This tutorial builds on this guide to [Create a very large FITS file from scratch](https://docs.astropy.org/en/stable/generated/examples/io/skip_create-large-fits.html) and shows how to buid a large fits file with multiple HDUs and with the big one not being the last. It is aimed at users already quite familiar with the FITS format.\n",
    "\n",
    "\n",
    "\n",
    "## Authors\n",
    "C. E. Brasseur\n",
    "\n",
    "## Learning Goals\n",
    "* Build a *large* FITS file (*large* means is too large to fit in memory all at once)\n",
    "* Access data from a *large* FITS file\n",
    "* Modify a *large* FITS file\n",
    "\n",
    "## Keywords\n",
    "Example, example, example\n",
    "\n",
    "## Companion Content\n",
    "LINK TO FITS DOCUMENTATION\n",
    "\n",
    "## Summary\n",
    "\n",
    "This is an advanced tutorial. If you don't want to know about the inner workings of the FITS format, just stop here. If you don't want to know but nevertheless neeed to, proceed with caution, that's how I started and now here I am writing this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd952d-56e3-4963-8540-f43020a7ea9e",
   "metadata": {},
   "source": [
    "## Building a large FITS file\n",
    "\n",
    "1. [Imports](#Imports)\n",
    "2. [Primary HDU](#Primary-HDU)\n",
    "3. [Large Image HDU](#Large-Image-HDU)\n",
    "4. [Large Table HDU](#Large-Table-HDU)\n",
    "5. [Adding an Extra Small HDU](#Adding-an-Extra-Small-HDU)\n",
    "6. [Cleanup](#Cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b583d",
   "metadata": {},
   "source": [
    "https://docs.astropy.org/en/stable/generated/examples/io/skip_create-large-fits.html\n",
    "\n",
    "https://fits.gsfc.nasa.gov/fits_standard.html\n",
    "\n",
    "https://docs.python.org/3/library/mmap.html#mmap.mmap.madvise\n",
    "\n",
    "https://docs.python.org/3/library/mmap.html#madvise-constants\n",
    "\n",
    "https://man7.org/linux/man-pages/man2/madvise.2.html\n",
    "\n",
    "https://github.com/astropy/astropy/issues/1380\n",
    "\n",
    "https://github.com/astropy/astropy/pull/7597\n",
    "\n",
    "https://github.com/astropy/astropy/pull/7926"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c21c1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "\n",
    "from mmap import MADV_SEQUENTIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97374113",
   "metadata": {},
   "source": [
    "And since we're building a huge file, we'll write a little function to give us the file size in a whatever units we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab165db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_file_size(path, unit=\"B\"):\n",
    "    \n",
    "    size = os.path.getsize(path)\n",
    "    \n",
    "    if unit==\"KB\":\n",
    "        size /= 1e3\n",
    "        fmt = '.1f'\n",
    "    elif unit==\"MB\":\n",
    "        size /= 1e6\n",
    "        fmt = '.1f'\n",
    "    elif unit==\"GB\":\n",
    "        size /= 1e9\n",
    "        fmt = '.1f'\n",
    "    elif unit==\"FITS\":\n",
    "        size //= 2880\n",
    "        unit = \"FITS block(s)\"\n",
    "        fmt = 'd'\n",
    "        \n",
    "    else:\n",
    "        unit = \"Bytes\"\n",
    "        fmt = 'd'\n",
    "        \n",
    "    print(f\"{size:{fmt}} {unit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc642cac",
   "metadata": {},
   "source": [
    "## Primary HDU\n",
    "\n",
    "We're going to build this up as a properly formated multi-extension FITS file, so before we get into the matter of greating a masive FITS file we will build a basic primary header and write that to the file that will become our monster FITS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f301e48-ab85-422d-817e-7926e4c1d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some header entries for important information\n",
    "primary_header_cards = [(\"ORIGIN\", 'Fancy Archive', \"Where the data came from\"),\n",
    "                        (\"DATE\", '2024-03-05',  \"Creation date\"),\n",
    "                        (\"MJD\", 60374, \"Creation date in MJD\"),\n",
    "                        (\"CREATOR\", 'Me',  \"Who created this file\")]  \n",
    "\n",
    "# Build the Primary HDU object and put it in an HDU list\n",
    "primary_hdu = fits.PrimaryHDU(header=fits.Header(primary_header_cards))\n",
    "hdu_list = fits.HDUList([primary_hdu])\n",
    "\n",
    "# Write the HDU list to file\n",
    "big_fits_fle = \"./patagotitan.fits\"\n",
    "hdu_list.writeto(big_fits_fle, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049580b1",
   "metadata": {},
   "source": [
    "Before we continue let's verify our (currently tiny) FITS file is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(big_fits_fle) as hdu_list:\n",
    "    hdu_list.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e14e379",
   "metadata": {},
   "source": [
    "Checking out the current size, we see it's one FITS block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714965ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_size(big_fits_fle)\n",
    "print_file_size(big_fits_fle, \"FITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf02d6",
   "metadata": {},
   "source": [
    "## Large Image HDU\n",
    "\n",
    "Here we are going to expand out FITS file to fit a large (40,000 x 40,000 pixel) image. This will cause the file to grow to ~13 GB in size. If that is too large for your system, adjust `array_dims` below. All of the steps still work as expected with smaller data, just there are simpler ways to do this if the whole FITS file fits in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b5a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_dims = [40_000, 40_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef407f",
   "metadata": {},
   "source": [
    "First we build an ImageHDU object with a small data array. The data in the array does not matter because we won't be using it, but the data type needs to be correct, and you need to know how many bytes per entry goes with that data type. In this case we are maing a `float64` array, so each entry uses 8 bytes of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((100, 100), dtype=np.float64)\n",
    "hdu = fits.ImageHDU(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bac0e2",
   "metadata": {},
   "source": [
    "Now we pull out just the header, and adjust the NAXIS keywords to matach our desired large aray dimensions. We also set an EXTNAME which is optional, but helpful because it allows us to refer to that extention by name as well as index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d874e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = hdu.header\n",
    "\n",
    "header[\"NAXIS2\"] = array_dims[0]\n",
    "header[\"NAXIS1\"] = array_dims[1]\n",
    "\n",
    "header[\"EXTNAME\"] = 'BIG_IMG' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943335ed",
   "metadata": {},
   "source": [
    "Now we write just the header to the end of our soon to balloon FITS file (at the end of this step it is temporarily NOT a valid FITS file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93568d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(big_fits_fle, 'ab') as FITSFLE:  # 'ab' means open to append bytes\n",
    "    FITSFLE.write(bytearray(header.tostring(), encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704c93b",
   "metadata": {},
   "source": [
    "Now we calculate the number of bytes we need for our large array, remembering the result needs to be a multiple of 2880 bytes to conform to the FITS standard. Note the multiplication by 8 because our array is of type `float64` this would be adjusted for different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a494a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arraysize_in_bytes = ((np.prod(array_dims)  * 8 + 2880 - 1) // 2880) * 2880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a970eb1",
   "metadata": {},
   "source": [
    "Now we need to expand the file by that many bytes. To do this we seek to the desired new end of the file and write a null byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cd09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelen = os.path.getsize(big_fits_fle) \n",
    "        \n",
    "with open(big_fits_fle, 'r+b') as FITSFLE:\n",
    "    FITSFLE.seek(filelen + arraysize_in_bytes - 1)\n",
    "    FITSFLE.write(b'\\0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c168f65",
   "metadata": {},
   "source": [
    "Now lets see how big our FITS file has become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_size(big_fits_fle, \"GB\")\n",
    "print_file_size(big_fits_fle, \"FITS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1e191",
   "metadata": {},
   "source": [
    "So just about 13 GB as expected, and a lot more FITS blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0910723e",
   "metadata": {},
   "source": [
    "### Filling the big array\n",
    "\n",
    "Now we have a big ol' empty array, so lets put some stuff in it.\n",
    "\n",
    "Add some stuff about he different ways of openeing fits files and how memmap is now critical etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list = fits.open(big_fits_fle, mode='update', memmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98664fb",
   "metadata": {},
   "source": [
    "That's what we expect, and also this is a point where you find out if you've messed up this operations. FITS files don't have indices up front so the computer just has to scan through it (in chunks of 2880) looking for more extensions. By default the Astropy fits module does not do this until necessary (LINK TO DOCS), so it's at the point where we call the info function that we find out if out FITS file is still valid. If this operations hangs, most likely the array size calculation is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac94786",
   "metadata": {},
   "source": [
    "We'll pull out the large data array, and then fill it in a loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = hdu_list[1].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42698c9a",
   "metadata": {},
   "source": [
    "If you are on a system with the `madvise` call (you're on your own figuring that out), you can set madvise to MADV_SEQUENTIAL for the data_array. This tells the memory mapping that you are going to be accessing the array in a sequential manner and allows it to be more efficient in how it handles memory allocation based on that. (Obviously don't set this if you aren't going to be accessing the array sequentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = fits.util._get_array_mmap(data_array)\n",
    "mm.madvise(MADV_SEQUENTIAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ca830",
   "metadata": {},
   "source": [
    "Now we fill the large array in blocks. We want the block size to comfortably fit in memory. The block_size I am using yields an ~1.3 GB array, adjust as your system requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 4000\n",
    "\n",
    "it = time()\n",
    "for i,j in enumerate(range(0, array_dims[0], block_size)):\n",
    "    sub_arr = np.ones((block_size,array_dims[1]))*i\n",
    "    data_array[j:j+block_size,:] = sub_arr\n",
    "    print(f\"{i}: {time()-it:.0f} sec\")\n",
    "    it = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900aa92e",
   "metadata": {},
   "source": [
    "Note the differing times for the loops. DO I KNOW WHY????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274cee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54875a56",
   "metadata": {},
   "source": [
    "### Checking the file contents\n",
    "\n",
    "So now we've theoretically filled the elephantine array, but we want to make sure it actually got filled and save. So we'll open the file in a non-editable mode and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list = fits.open(big_fits_fle, mode='denywrite', memmap=True)\n",
    "\n",
    "data_array = hdu_list[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba09b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = time()\n",
    "for i,j in enumerate(range(0, array_dims[0], block_size)):\n",
    "    print(f\"{i}: Data match is {(data_array[j:j+block_size,:] == i).all()}: {time()-it:.0f} sec\")\n",
    "    it = time()\n",
    "    \n",
    "hdu_list.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7298d9",
   "metadata": {},
   "source": [
    "## Large Table HDU\n",
    "\n",
    "In the last section we expanded our FITS file to add a colossal image extension, in this section we will do the same for a table extension. The method is similar, but with a few key differences.\n",
    "\n",
    "As with the image the data is not important but the data types are. In particular, the maximum string length for columns cannot be changed one the fly (since the memory has been allocated and is fixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4c31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tbl = Table(names=[\"Name\", \"Population\", \"Prince\", \"Years since fall\", \"Imports\", \"Exports\"],\n",
    "                  dtype=['U128', int, 'U128', np.float64, 'U2048', 'U2048'],\n",
    "                  rows=[[\"Vangaveyave\", 1297382, \"Oriana\", 34.6, \"wine, cheese\", \"ahalo cloth, pearls, foamwork\"],\n",
    "                        [\"Azilint\", 50000, \"n/a\", 92.3, \"none\", \"none\"],\n",
    "                        [\"Amboloyo\", 50937253, \"Rufus\", 1504.2, \"pears, textiles, spices\", \"wine, timber\"]])\n",
    "\n",
    "table_hdu = fits.BinTableHDU(data=small_tbl)\n",
    "table_hdu.header[\"EXTNAME\"] = \"BIG_TABLE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be82b91",
   "metadata": {},
   "source": [
    "The header for this table HDU gives us the information to determine how many bytes we need for our mammoth table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528aa250",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_hdu.header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ee86c",
   "metadata": {},
   "source": [
    "The `NAXIS#` keywords hold the dimensions of the table where `NAXIS1` is the length of a single table row in bytes and `NAXIS2` is the number of rows in the table. So to get the total size of the jumbo table in bytes we simply multiply `NAXIS1` by the number of rows desired (adjusting for FITS blocksize). I'm choosing a million rows which is about 4GB, adjust as necessary for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1_000_000\n",
    "tablesize_in_bytes = ((table_hdu.header[\"NAXIS1\"]*num_rows + 2880 - 1) // 2880) * 2880"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a95ad0",
   "metadata": {},
   "source": [
    "Now we adjust the `NAXIS2` keyword to match our new table length and write just the header to the end of our towering FITS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_hdu.header[\"NAXIS2\"] = num_rows\n",
    "\n",
    "with open(big_fits_fle, 'ab') as FITSFLE:\n",
    "    FITSFLE.write(bytearray(table_hdu.header.tostring(), encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea704c",
   "metadata": {},
   "source": [
    "Before we expand the file, lets remind ourself of the current filesize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86193af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_size(big_fits_fle, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9579e19",
   "metadata": {},
   "source": [
    "Now, just as for the vast data array, we seek `tablesize_in_bytes` beyond the current end of the file and write a null byte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "filelen = os.path.getsize(big_fits_fle)\n",
    "\n",
    "with open(big_fits_fle, 'r+b') as FITSFLE:\n",
    "    FITSFLE.seek(filelen + tablesize_in_bytes - 1)\n",
    "    FITSFLE.write(b'\\0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180aa5d",
   "metadata": {},
   "source": [
    "And we can see that the filesize has indeed increased by about 4GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_file_size(big_fits_fle, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ff84f",
   "metadata": {},
   "source": [
    "### Adding data to the titanic table\n",
    "\n",
    "We can now open the prodigeous FITS file in update mode and fill in our table. Note that this time we don't advise the memory mapper we will be accessing the memory in sequential order, because we are not doing that.\n",
    "\n",
    "CHANGE THIS NOW I KNOW IT'S STORED ROW BY ROW\n",
    "\n",
    "\n",
    "ALSO CAN FILL ROW BY ROW\n",
    "\n",
    "In [6]: hdu.data\n",
    "Out[6]: \n",
    "FITS_rec([(1, 1., 'c'), (2, 2., 'd'), (3, 3., 'e')],\n",
    "         dtype=(numpy.record, [('a', '<i8'), ('b', '<f8'), ('c', 'S1')]))\n",
    "\n",
    "In [7]: hdu.data[0]\n",
    "Out[7]: (1, 1.0, 'c')\n",
    "\n",
    "In [8]: hdu.data[0] = (5, 5, 'f')\n",
    "\n",
    "In [9]: hdu.data\n",
    "Out[9]: \n",
    "FITS_rec([(5, 5., 'f'), (2, 2., 'd'), (3, 3., 'e')],\n",
    "         dtype=(numpy.record, [('a', '<i8'), ('b', '<f8'), ('c', 'S1')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fecbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list = fits.open(big_fits_fle, mode='update', memmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1641bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859498d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = hdu_list[\"BIG_TABLE\"].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490c29a",
   "metadata": {},
   "source": [
    "It's easier to update FITS column-wise rather than row-wise so we'll start by updating a couple of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = time()\n",
    "table_data[\"Years since fall\"] = np.linspace(5,1000,1000000)\n",
    "print(f\"Float column: {time()-it:.0f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04225b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = time()\n",
    "table_data[\"Exports\"] = [\"Magic\"]*1_000_000\n",
    "print(f\"String column: {time()-it:.0f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e584ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = time()\n",
    "hdu_list.flush()\n",
    "print(f\"Flushing: {time()-it:.0f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523a375",
   "metadata": {},
   "source": [
    "We can of course add data on a row by row basis, however, to do that we have to access each field individually. To demonstrate that I'll add in some data from our original small table in a couple of random rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [0,2]:\n",
    "    for col in small_tbl.colnames:\n",
    "        table_data[col][i*50000] = small_tbl[col][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2268927",
   "metadata": {},
   "source": [
    "When we close the file it has to flush our new data to disk, so this can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b554b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = time()\n",
    "hdu_list.close()\n",
    "print(f\"Closing: {time()-it:.0f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0446f16a",
   "metadata": {},
   "source": [
    "### Checking our data\n",
    "\n",
    "Now let's again open up our behemothic FITS file and check that the daa we just loaded in is still there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0351aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list = fits.open(big_fits_fle, mode='denywrite', memmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc349a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu_list.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fa7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = hdu_list[\"BIG_TABLE\"].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e5baa",
   "metadata": {},
   "source": [
    "Checking the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52842277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f6a632",
   "metadata": {},
   "source": [
    "Looking at the one row later on we also put data in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ef669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_data[100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63363f7",
   "metadata": {},
   "source": [
    "Note how accessing the middle of the array takes longer than accessing the start."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f4502",
   "metadata": {},
   "source": [
    "## Adding an Extra Small HDU\n",
    "\n",
    "The last thing we will do is add another small HDU to the oversize FITS file. We can do this in the usual way because the extension we are adding is of a normal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaddafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_hdu = fits.ImageHDU(data=np.random.random((10,10)))\n",
    "small_hdu.header[\"EXTNAME\"] = \"MINI_IMG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb74ccb",
   "metadata": {},
   "source": [
    "Because we don't have to do anything funky with the file size we can just open the mighty FITS file in `append` mode and write the whole HDU, and it is a very fast operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(big_fits_fle, mode='append', memmap=True) as hdu_list:\n",
    "    hdu_list.append(small_hdu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf267e",
   "metadata": {},
   "source": [
    "And now if we open the mondo FITS file we can see that additional extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(big_fits_fle, mode='denywrite', memmap=True) as hdu_list:\n",
    "    hdu_list.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849a3b4",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Lastly, we'll remove the behemothic file we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(big_fits_fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f34dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
